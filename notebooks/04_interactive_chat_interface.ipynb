{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiskirB/B5W6-Intelligent-Complaint-Analysis/blob/main/04_interactive_chat_interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZX8qS7wpRD2",
        "outputId": "e12813e7-c16b-4591-d082-be60049e7591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 🧩 STEP 1: Install dependencies\n",
        "!pip install -q streamlit pyngrok sentence-transformers faiss-cpu transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RraOaIZCpm1V",
        "outputId": "edb01871-70ba-49fd-e37b-5ac1bf492adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted!\")\n",
        "\n",
        "# Optional: Verify your vector store path exists\n",
        "# Example: !ls \"/content/drive/MyDrive/B5W6-Intelligent-Complaint-Analysis/vector_store/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vqv9lLrBpI_",
        "outputId": "befffe44-d1f3-4fac-af2e-b0831d0114cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔐 Enter your Hugging Face token: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Logged in to Hugging Face successfully!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for Hugging Face token (hidden input)\n",
        "hf_token = getpass(\"🔐 Enter your Hugging Face token: \")\n",
        "\n",
        "if hf_token:\n",
        "    os.environ['HF_TOKEN'] = hf_token\n",
        "    login(token=hf_token)\n",
        "    print(\"✅ Logged in to Hugging Face successfully!\")\n",
        "else:\n",
        "    print(\"❌ Hugging Face token not provided.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "59kfdzBWpU8z",
        "outputId": "4257422c-b89f-42ea-ebb1-80c288e9fcdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d73b91a15b4b8115a2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d73b91a15b4b8115a2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# %%writefile app.py # Keep this if you're writing to app.py in Colab\n",
        "\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "\n",
        "# --- Hugging Face Login ---\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "\n",
        "# File paths\n",
        "INDEX_PATH = \"/content/drive/MyDrive/B5W6-Intelligent-Complaint-Analysis/vector_store/faiss_index.index\"\n",
        "META_PATH = \"/content/drive/MyDrive/B5W6-Intelligent-Complaint-Analysis/vector_store/metadata.pkl\"\n",
        "\n",
        "# Load resources\n",
        "try:\n",
        "    index = faiss.read_index(INDEX_PATH)\n",
        "    with open(META_PATH, \"rb\") as f:\n",
        "        metadata = pickle.load(f)\n",
        "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    generator = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\", device=0 if torch.cuda.is_available() else -1)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error loading resources: {e}\")\n",
        "\n",
        "def retrieve_similar_chunks(query, top_k=5):\n",
        "    query_emb = embed_model.encode([query])[0].astype(\"float32\")\n",
        "    D, I = index.search(np.array([query_emb]), top_k)\n",
        "    results = []\n",
        "    for i, idx in enumerate(I[0]):\n",
        "        entry = metadata[idx]\n",
        "        results.append({\n",
        "            \"rank\": i + 1,\n",
        "            \"product\": entry[\"product\"],\n",
        "            \"text\": entry[\"text\"]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def format_prompt(chunks, question):\n",
        "    context_blocks = \"\\n\\n\".join([f\"[{i+1}] {c['text']}\" for i, c in enumerate(chunks)])\n",
        "    return f\"\"\"You are a financial analyst assistant for CrediTrust. Your task is to answer questions about customer complaints.\n",
        "\n",
        "Use only the following retrieved complaint excerpts to formulate your answer. If the context does not contain enough information, respond with \\\"I don't have enough information to answer that.\\\"\n",
        "\n",
        "Context:\n",
        "{context_blocks}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def generate_response(prompt):\n",
        "    result = generator(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
        "    return result.split(\"Answer:\")[-1].strip() if \"Answer:\" in result else result.strip()\n",
        "\n",
        "def answer_question(query):\n",
        "    chunks = retrieve_similar_chunks(query)\n",
        "    if not chunks:\n",
        "        return \"No relevant chunks found.\", \"\"\n",
        "    prompt = format_prompt(chunks, query)\n",
        "    answer = generate_response(prompt)\n",
        "    sources = \"\\n\\n\".join([f\"🔹 {c['product']}:\\n{c['text']}\" for c in chunks])\n",
        "    return answer, sources\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🧠 CrediTrust Complaint QA Chat\")\n",
        "    gr.Markdown(\"Ask any question related to customer complaints.\")\n",
        "\n",
        "    query_input = gr.Textbox(lines=2, label=\"💬 Ask your question about complaints:\")\n",
        "    answer_output = gr.Textbox(label=\"📌 Answer\")\n",
        "    source_output = gr.Textbox(label=\"📚 Retrieved Sources\")\n",
        "\n",
        "    feedback_msg = gr.Textbox(visible=False)  # optional confirmation message\n",
        "    feedback_state = gr.State(value=None)  # stores the last answer to give feedback on\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Ask\")\n",
        "        clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    with gr.Row():\n",
        "        like_btn = gr.Button(\"👍 Helpful\")\n",
        "        dislike_btn = gr.Button(\"👎 Not Helpful\")\n",
        "\n",
        "    def handle_query(query):\n",
        "        answer, sources = answer_question(query)\n",
        "        return answer, sources, answer  # store answer for feedback\n",
        "\n",
        "    def give_feedback(feedback, answer):\n",
        "        # Replace with logging, saving to file, or storing in db\n",
        "        print(f\"User feedback: {feedback} | Answer: {answer}\")\n",
        "        return f\"✅ Feedback recorded: {feedback}\"\n",
        "\n",
        "    submit_btn.click(handle_query, inputs=[query_input], outputs=[answer_output, source_output, feedback_state])\n",
        "    clear_btn.click(lambda: (\"\", \"\", None), outputs=[answer_output, source_output, feedback_state])\n",
        "\n",
        "    like_btn.click(lambda ans: give_feedback(\"Helpful\", ans), inputs=[feedback_state], outputs=[feedback_msg])\n",
        "    dislike_btn.click(lambda ans: give_feedback(\"Not Helpful\", ans), inputs=[feedback_state], outputs=[feedback_msg])\n",
        "\n",
        "demo.launch(share=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMJw+rpsmdUT/my0Ouw/skx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}